{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Models for Question Retrieval\n",
    "\n",
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from gensim import corpora\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Goeievraag corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train questions:  509605\n",
      "Number of train answers:  509605\n"
     ]
    }
   ],
   "source": [
    "goeievraag_path = '/roaming/fkunnema/goeievraag' # goeievraag path\n",
    "# loading tokenized and 'stopworded' questions, and lowercase them.\n",
    "train_question_path = os.path.join(goeievraag_path, 'exp_similarity', 'unseeded_questions.tok.txt')\n",
    "with open(train_question_path) as f:\n",
    "    doc = f.read()\n",
    "\n",
    "questions = [question.lower().split() for question in doc.split('\\n')[:-1]]\n",
    "print('Number of train questions: ', str(len(questions)))\n",
    "\n",
    "train_answer_path = os.path.join(goeievraag_path, 'exp_similarity', 'unseeded_answers.tok.txt')\n",
    "with open(train_answer_path) as f:\n",
    "    doc = f.read()\n",
    "\n",
    "answers = [question.lower().split() for question in doc.split('\\n')[:-1]]\n",
    "print('Number of train answers: ', str(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing and saving dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-01 15:18:06,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-08-01 15:18:07,116 : INFO : adding document #10000 to Dictionary(16241 unique tokens: ['geschatte', 'verkochte', 'yong', 'geitenkaas', 'bloemenwater']...)\n",
      "2018-08-01 15:18:07,426 : INFO : adding document #20000 to Dictionary(25276 unique tokens: ['verkochte', 'yong', 'ml', 'gv-accounts', 'bloemenwater']...)\n",
      "2018-08-01 15:18:07,743 : INFO : adding document #30000 to Dictionary(32844 unique tokens: ['verkochte', 'yong', 'ml', 'gv-accounts', 'bloemenwater']...)\n",
      "2018-08-01 15:18:08,031 : INFO : adding document #40000 to Dictionary(39080 unique tokens: ['verkochte', 'yong', 'ml', 'gv-accounts', 'bloemenwater']...)\n",
      "2018-08-01 15:18:08,317 : INFO : adding document #50000 to Dictionary(44782 unique tokens: ['ml', 'smiling', 'names', 'mbo', 'inrit']...)\n",
      "2018-08-01 15:18:08,599 : INFO : adding document #60000 to Dictionary(50108 unique tokens: ['ml', 'smiling', 'names', 'mbo', 'inrit']...)\n",
      "2018-08-01 15:18:08,884 : INFO : adding document #70000 to Dictionary(55193 unique tokens: ['ml', 'smiling', 'names', 'mbo', 'inrit']...)\n",
      "2018-08-01 15:18:09,175 : INFO : adding document #80000 to Dictionary(60085 unique tokens: ['stadskanaal', 'ml', 'smiling', 'names', 'mbo']...)\n",
      "2018-08-01 15:18:09,488 : INFO : adding document #90000 to Dictionary(64793 unique tokens: ['stadskanaal', 'ml', 'smiling', 'names', 'mbo']...)\n",
      "2018-08-01 15:18:09,788 : INFO : adding document #100000 to Dictionary(69475 unique tokens: ['stadskanaal', 'ml', 'smiling', 'names', 'mbo']...)\n",
      "2018-08-01 15:18:10,086 : INFO : adding document #110000 to Dictionary(73849 unique tokens: ['stadskanaal', 'ml', 'smiling', 'names', 'mbo']...)\n",
      "2018-08-01 15:18:10,385 : INFO : adding document #120000 to Dictionary(77930 unique tokens: ['stadskanaal', 'geintresseerd', 'ml', 'bestevriendin', 'smiling']...)\n",
      "2018-08-01 15:18:10,652 : INFO : adding document #130000 to Dictionary(80845 unique tokens: ['stadskanaal', 'geintresseerd', 'ml', 'bestevriendin', 'smiling']...)\n",
      "2018-08-01 15:18:10,933 : INFO : adding document #140000 to Dictionary(84156 unique tokens: ['stadskanaal', 'geintresseerd', 'ml', 'bestevriendin', 'smiling']...)\n",
      "2018-08-01 15:18:11,229 : INFO : adding document #150000 to Dictionary(87556 unique tokens: ['geintresseerd', 'ml', 'smiling', 'middenbouw', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:11,508 : INFO : adding document #160000 to Dictionary(90882 unique tokens: ['geintresseerd', 'ml', 'smiling', 'middenbouw', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:11,786 : INFO : adding document #170000 to Dictionary(93986 unique tokens: ['geintresseerd', 'ml', 'smiling', 'middenbouw', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:12,068 : INFO : adding document #180000 to Dictionary(97257 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:12,354 : INFO : adding document #190000 to Dictionary(100526 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:12,647 : INFO : adding document #200000 to Dictionary(103686 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:12,942 : INFO : adding document #210000 to Dictionary(106832 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:13,232 : INFO : adding document #220000 to Dictionary(109896 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:13,527 : INFO : adding document #230000 to Dictionary(112967 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:13,822 : INFO : adding document #240000 to Dictionary(116012 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:14,116 : INFO : adding document #250000 to Dictionary(119032 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:14,415 : INFO : adding document #260000 to Dictionary(122040 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:14,718 : INFO : adding document #270000 to Dictionary(125070 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:15,019 : INFO : adding document #280000 to Dictionary(128140 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:15,323 : INFO : adding document #290000 to Dictionary(131249 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:15,625 : INFO : adding document #300000 to Dictionary(134233 unique tokens: ['geintresseerd', 'expliciteren', 'ml', 'smiling', 'middenbouw']...)\n",
      "2018-08-01 15:18:15,932 : INFO : adding document #310000 to Dictionary(137227 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:16,243 : INFO : adding document #320000 to Dictionary(140156 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:16,550 : INFO : adding document #330000 to Dictionary(142999 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:16,860 : INFO : adding document #340000 to Dictionary(145907 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:17,171 : INFO : adding document #350000 to Dictionary(148757 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:17,465 : INFO : adding document #360000 to Dictionary(151350 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:17,759 : INFO : adding document #370000 to Dictionary(154049 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:18,042 : INFO : adding document #380000 to Dictionary(156671 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:18,330 : INFO : adding document #390000 to Dictionary(159321 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:18,615 : INFO : adding document #400000 to Dictionary(162093 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:18,900 : INFO : adding document #410000 to Dictionary(164953 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:19,188 : INFO : adding document #420000 to Dictionary(167802 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:19,470 : INFO : adding document #430000 to Dictionary(170300 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:19,752 : INFO : adding document #440000 to Dictionary(172909 unique tokens: ['geintresseerd', 'usb-server', 'expliciteren', 'ml', 'smiling']...)\n",
      "2018-08-01 15:18:20,047 : INFO : adding document #450000 to Dictionary(175381 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:20,333 : INFO : adding document #460000 to Dictionary(177959 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:20,618 : INFO : adding document #470000 to Dictionary(180698 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:20,905 : INFO : adding document #480000 to Dictionary(183543 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:21,189 : INFO : adding document #490000 to Dictionary(186142 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:21,470 : INFO : adding document #500000 to Dictionary(188425 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...)\n",
      "2018-08-01 15:18:21,743 : INFO : built Dictionary(190600 unique tokens: ['geintresseerd', 'ml', 'veronderstellen', 'muziekbron', 'filtreerpapier']...) from 509605 documents (total 6065916 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(questions)\n",
    "#dictionary.save('goeievraag.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question unigrams P(w | C)\n",
    "tokens = []\n",
    "for question in questions:\n",
    "    for token in question:\n",
    "        tokens.append(token)\n",
    "\n",
    "Q_len = float(len(tokens))\n",
    "w_Q = dictionary.doc2bow(tokens)\n",
    "w_Q = [(dictionary[w[0]], w[1]/Q_len) for w in w_Q]\n",
    "\n",
    "with open('prob_w_Q.txt', 'w') as f:\n",
    "    for w, prob in w_Q:\n",
    "        f.write(' '.join([w, str(prob), '\\n']))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Base():\n",
    "    def __init__(self, training, prob_w_C, alpha):\n",
    "        self.training = training\n",
    "        self.prob_w_C = prob_w_C\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def score(self, query, question):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    def rank(self, query, n=10):\n",
    "        ranking = []\n",
    "        for question in self.training:\n",
    "            prob = self.score(query, question)\n",
    "            ranking.append((question, prob))\n",
    "        \n",
    "        ranking = sorted(ranking, key=lambda x: x[0], reverse=True)\n",
    "        return ranking[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LM(Base):\n",
    "    def __init__(self, training, prob_w_C, alpha):\n",
    "        Base.__init__(self, training, prob_w_C, alpha) \n",
    "        \n",
    "    def score(self, query, question):\n",
    "        Q = pd.Series(question)\n",
    "        prob = 0.0\n",
    "        for w in query:\n",
    "            w_Q = float(Q[Q == w].count()) / Q.count()\n",
    "\n",
    "            try:\n",
    "                w_C = self.prob_w_C[w.lower()]\n",
    "            except:\n",
    "                w_C = 0.0\n",
    "            \n",
    "            prob += np.log(((1-alpha) * w_Q) + (alpha * w_C))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['i', 'like', 'bananas'], -8.38033400869904),\n",
       " (['i', 'like', 'apples'], -8.38033400869904),\n",
       " (['i', 'hate', 'aubergine'], -6.343452081438)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = ['i like bananas', 'i like apples', 'i hate aubergine']\n",
    "training = map(lambda s: s.split(), training)\n",
    "\n",
    "prob_w_C = {\n",
    "    'i': 0.1,\n",
    "    'like': 0.2,\n",
    "    'bananas': 0.15,\n",
    "    'apples': 0.15,\n",
    "    'hate': 0.2,\n",
    "    'aubergine': 0.1,\n",
    "    'pineapple': 0.1\n",
    "}\n",
    "\n",
    "alpha = 0.2\n",
    "\n",
    "lm = LM(training, prob_w_C, alpha)\n",
    "lm.rank('i hate pineapple'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TM(Base):\n",
    "    def __init(self, training, prob_w_C, prob_w_t, alpha):\n",
    "        Base.__init__(self, training, prob_w_C, alpha) \n",
    "        self.prob_w_t = prob_w_t\n",
    "        \n",
    "    def score(query, question):\n",
    "        Q = pd.Series(question)\n",
    "        prob = 0.0\n",
    "        for w in query:\n",
    "            try:\n",
    "                w_C = self.prob_w_C[w.lower()]\n",
    "            except:\n",
    "                w_C = 0.0\n",
    "            \n",
    "            w_Q = 0.0\n",
    "            for t in question:\n",
    "                try:\n",
    "                    w_t = prob_w_t[w][t]\n",
    "                except:\n",
    "                    w_t = 0.0\n",
    "                \n",
    "                t_Q = float(Q[Q == t].count()) / Q.count()\n",
    "                w_Q += (w_t * t_Q)\n",
    "            prob += np.log(((1-alpha) * w_Q) + (alpha * w_C))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation-based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TRLM(Base):\n",
    "    def __init__(self, training, prob_w_C, prob_w_t, alpha, sigma):\n",
    "        Base.__init__(self, training, prob_w_C, alpha) \n",
    "        self.prob_w_t = prob_w_t\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def score(self, query, question):\n",
    "        Q = pd.Series(question)\n",
    "        prob = 0.0\n",
    "        for w in query:\n",
    "            try:\n",
    "                w_C = self.prob_w_C[w.lower()]\n",
    "            except:\n",
    "                w_C = 0.0\n",
    "            \n",
    "            ml_w_Q = float(Q[Q == w].count()) / Q.count()\n",
    "            mx_w_Q = 0.0\n",
    "            for t in question:\n",
    "                try:\n",
    "                    w_t = prob_w_t[w][t]\n",
    "                except:\n",
    "                    w_t = 0.0\n",
    "                \n",
    "                t_Q = float(Q[Q == t].count()) / Q.count()\n",
    "                mx_w_Q += (w_t * t_Q)\n",
    "            w_Q = (sigma * mx_w_Q) + ((1-sigma) * ml_w_Q)\n",
    "            prob += np.log(((1-alpha) * w_Q) + (alpha * w_C))\n",
    "        return prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
