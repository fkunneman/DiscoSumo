{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download globe vectors on [the stanford website](https://nlp.stanford.edu/projects/glove/). For now, the vectors are available at `/exp2/tcastrof/glove`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_PATH = '/exp2/tcastrof/glove/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading vectors with 300 dimensons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(GLOVE_PATH) as f:\n",
    "    lookup = f.read().split('\\n')[:-1]\n",
    "    lookup = dict(map(lambda word: (word.split()[0], np.array([float(i) for i in word.split()[1:]])), lookup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity among two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(word1, word2):\n",
    "    similarity = -1\n",
    "    if word1.lower() not in lookup:\n",
    "        print('No embedding for word 1')\n",
    "    elif word2.lower() not in lookup:\n",
    "        print('No embedding for word 2')\n",
    "    else:\n",
    "        similarity = 1 - spatial.distance.cosine(lookup[word1], lookup[word2])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence cosine similarity by averaging word embeddings of each sentence. `snt1` and `snt2` are tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_cosine(snt1, snt2):\n",
    "    similarity = -1\n",
    "    try:\n",
    "        snt1_emb = np.array(list(map(lambda word: lookup[word.lower()], snt1)))\n",
    "        snt2_emb = np.array(list(map(lambda word: lookup[word.lower()], snt2)))\n",
    "        avg1 = np.mean(snt1_emb, axis=0)\n",
    "        avg2 = np.mean(snt2_emb, axis=0)\n",
    "        similarity = 1 - spatial.distance.cosine(avg1, avg2)\n",
    "    except:\n",
    "        print('Embedding not found for a word in sentence 1 or 2.')\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8858211415622662"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt1 = ['I', 'like', 'bananas']\n",
    "snt2 = ['I', 'hate', 'bananas']\n",
    "\n",
    "sentence_cosine(snt1, snt2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
